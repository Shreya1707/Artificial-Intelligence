# -*- coding: utf-8 -*-
"""AIcollegeproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kXoB6Q-cWri0-y8p4Di-CEt5-pC1akiv
"""

#import required libraries
#Load data
#Divide it into training and testing
#Further divide training to training and validation
#Change in dimension
#Normalization
#Built the CNN Model
#Train model
#Test and evaluate model
#Report
#Save model
#Modified National Institute of Standards and Technology database.

#Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples 
#and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. 

#CNN:Used for image recognition
#CNN: When we give an image as input,it will divide this image into many sub images using matrix this is called convolutional stack
#by multiplying matrix with single filter after which we get a filter image which is called as feature map
#So just like this we use different filters we can create many images
#But since we have many images,size is too large so we need to compress
#hence we use pooling to compress
#this process is also called sub sampling
#0:black and 255 means white
#we normalize image by 255 so the numbers will be small and the computation becomes easier and faster.

#simulation of human intelligence processes by machines, especially computer systems. Specific applications of AI include expert systems, natural language processing, speech recognition and machine vision.
#AI is basically the study of training your machine(computers) to mimic a human brain and its thinking capabilities”
#Machine Learning:

#Machine Learning is basically the study/process which provides the system(computer) to learn automatically on its own 
#through experiences it had and improve accordingly without being explicitly programmed. ML is an application or subset of AI
# Deep Learning is basically a sub-part of the broader family of Machine Learning which makes use of Neural Networks
#(similar to the neurons working in our brain) to mimic human brain-like behavior. 

'''
Some examples of AI include:
Machine learning and deep learning are both types of AI. In short, machine learning is AI that can automatically adapt with minimal human interference. 
Deep learning is a subset of machine learning that uses artificial neural networks to mimic the learning process of the human brain.
Machine learning is used for a wide range of applications, such as regression analysis, classification, and clustering. 	
Deep learning, on the other hand, is mostly used for complex tasks such as image and speech recognition, natural language processing, and autonomous systems.
Convolutional neural networks are composed of multiple layers of artificial neurons. Artificial neurons, a rough imitation of their biological counterparts, are mathematical functions that calculate the weighted sum of multiple inputs and outputs an activation value. When you input an image in a ConvNet, each layer generates several activation functions that are passed on to the next layer.

The first layer usually extracts basic features such as horizontal or diagonal edges. This output is passed on to the next layer which detects 
more complex features such as corners or combinational edges. 
As we move deeper into the network it can identify even more complex features such as objects, faces, etc.

Similar to the Convolutional Layer, the Pooling layer is responsible for reducing the spatial size of the Convolved Feature. 
This is to decrease the computational power required to process the data by reducing the dimensions. 
There are two types of pooling average pooling and max pooling

Max Pooling is we find the maximum value of a pixel from a portion of the image covered by the kernel. Max Pooling also performs as a Noise Suppressant. It discards the noisy activations altogether and also performs de-noising along with dimensionality reduction.

On the other hand, Average Pooling returns the average of all the values from the portion of the image covered by the Kernel. 
Average Pooling simply performs dimensionality reduction as a noise suppressing mechanism. 
Hence, we can say that Max Pooling performs a lot better than Average Pooling.

Padding: A padding layer is typically added to ensure that the outer boundaries of the input layer doesn’t lose its features when the convolution 
operation is applied. It is also done to adjust the size of the input. So in most cases a Zero Padding is applied.
Activation function:It’s just a thing function that you use to get the output of node. It is also known as Transfer Function.
It is used to determine the output of neural network like yes or no.

Flatten: The flatten layer basically takes the current pooling layer output and it converts it into the format which is required for the Fully connected layer.

The fully connected layer has a Softmax Activation function at the end which ensures that the sum of output probabilities from the Fully Connected Layer is 1.
The fully connected layer is the final piece of the puzzle. It basically takes the high level feature maps(nose, ears, etc.) as the input and decides what the output category would be.
CNN, short for Convolutional Neural Network, is a type of neural network commonly used in deep learning for image recognition, computer vision, and natural language processing tasks.

The key feature of CNNs is the use of convolutional layers, which perform a mathematical operation called convolution on the input data. This operation extracts local features of the input data by sliding a small window called a kernel over the input data and computing dot products between the kernel and the input data. The resulting output is a feature map, which represents the presence of the feature at each position in the input.

CNNs also typically use pooling layers to reduce the size of the feature maps and to make the model more robust to variations in the input data. Other common layers in CNNs include activation layers, which introduce nonlinearity into the model, and fully connected layers, which combine the features from the convolutional layers and map them to the output.

CNNs have been shown to be highly effective for a wide range of image and speech recognition tasks, and have been used in many state-of-the-art models for these tasks.
CNN, short for Convolutional Neural Network, is a type of neural network commonly used in deep learning for image recognition, computer vision, and natural language processing tasks.

The key feature of CNNs is the use of convolutional layers, which perform a mathematical operation called convolution on the input data. 
This operation extracts local features of the input data by sliding a small window called a kernel over the input data 
and computing dot products between the kernel and the input data. The resulting output is a feature map, 
which represents the presence of the feature at each position in the input.
CNNs also typically use pooling layers to reduce the size of the feature maps and to make the model more robust to variations in the input data. 
Other common layers in CNNs include activation layers, which introduce nonlinearity into the model, 
and fully connected layers, which combine the features from the convolutional layers and map them to the output.
CNNs have been shown to be highly effective for a wide range of image and speech recognition tasks, and have been used in many state-of-the-art models 
for these tasks.
'''
import numpy as np
import matplotlib.pyplot as plt #show images
import tensorflow as tf #keras is high level api of tf && tf is a library for machine learning
import keras #build CNN works on tenserflow
(X_train, y_train), (X_test, y_test)=tf.keras.datasets.fashion_mnist.load_data()

#divide dataset into 4 parts
X_train.shape,y_train.shape, X_test.shape,y_test.shape 
#height:28 pixel, width:28pixel, 60000:no of images , we hv stored labels in y_train so we hv 60000 labels
#x_train is a 3-D matrix which has many matrices of numbers in it
#where each matrix represents an image

class_labels = [	"T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal", "Shirt",	"Sneaker",	"Bag",	"Ankle boot"]
class_labels

X_train = np.expand_dims(X_train,-1) #-1 means at last idx #expanded dimensions from 3 to 4 as CNN requires 4 dimensions.We add 1 dimnsion at end 1 is for black and 3 is for coloured images

X_test=np.expand_dims(X_test,-1)#expanded dimensions from 3 to 4 .If we dont do it,we will get errors

X_train = X_train/255 #normalization so that all values r in range from 0 to 1
X_test= X_test/255 #Feature scaling In image processing, it is common to normalize the pixel values to a range 
#between 0 and 1 before feeding them into a machine learning model. 
#This is done to ensure that the input data has a consistent scale, which can help the optimization algorithm converge faster 
#and avoid issues like vanishing or exploding gradients.

from sklearn.model_selection import  train_test_split
X_train,X_Validation,y_train,y_Validation=train_test_split(X_train,y_train,test_size=0.2,random_state=2020) 
#divided the training images of 60000 into 48000 training and 12000 images for validation
#validation is done along with training means while training we r testing
#where as testing dataset is used to test after building CNN 
#0.2 means 20% data required for validation
#The random state hyperparameter is used to control the randomness involved in machine learning models
#The random_state=2020 parameter ensures that the same random split is produced each time the code is run.
#By setting random_state to a fixed value, we can ensure that the data split is reproducible. This is particularly important when sharing code or when conducting experiments that require the data split to remain constant.

X_train.shape,X_Validation.shape,y_train.shape,y_Validation.shape

model=keras.models.Sequential([
                         keras.layers.Conv2D(filters=32,kernel_size=3,strides=(1,1),padding='valid',activation='relu',input_shape=[28,28,1]),
                         #We need activation functions such as ReLU in neural networks because they introduce non-linearity to the output of the network. Without activation functions, the output of a neural network would be a linear function of the input, which would limit the power of the network to learn complex patterns in data.
                         #2D coz we hv 2D images
                         #3 layers: Convolutional,Pooling,Fully connected
                         #ReLU, which stands for Rectified Linear Unit, is an activation function commonly used in deep learning neural networks.
                         #no of filters=32 means one image is converted in multiple diff images
                         #kernel size=3
                         #Filters represent the number of output channels after convolution has been performed
                         #Kernel represents the size of a convolution filter being used to perform convolution on the image
                         #strides:no of pixel shifts 
                         #Padding is a process of adding zeros to the input matrix symmetrically.
                         #input shape=28 rows and 28 columns
                         #1 channel no means black and white
                         #if it was 3 it means coloured image
                         #creating an array of 2x2 and pick biggest value so it continues this process
                         #padding is added to the outer frame of the image to allow for more space for the filter to cover in the image
                         #activation: relu=> when we apply filters we can get value positive or negative
                         #relu=>positive value is same but a negative or 0 is treated as 0
                         keras.layers.MaxPooling2D(pool_size=(2,2)),
                         #MAXpooling is used to decrease size of image
                         #Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map.
                         # The results are down sampled or pooled feature maps that highlight the most present feature in the patch, 
                         #The pool size just takes a pool of 2x2 pixels, finds the sum of them and puts them into one pixel.
                         # Hence converting 2x2 pixels to 1x1 pixel, encoding it.
                         keras.layers.Flatten(),
                         #convert all the images into a single vector is flatten function's use for fully connected layers
                         keras.layers.Dense(units=128,activation='relu'),
                         #units is no of neurons=128
                         #dense is used to train cnn model
                         keras.layers.Dense(units=10,activation='softmax')
                         #A fully connected layer is a layer in which each neuron is connected to every neuron in the previous layer.
                         #10 neurons and softmax tells us probability of each image that how much it is correctly predicted
                         #The output of a Softmax is a vector (say v) with probabilities of each possible outcome.
])

model.summary()

model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
#optimisers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses and maximize accuracy
#we hv so many optimisers like gradient descent
#optimisers: adam,adagrad,adadelta,gradient descent
#Adam is the best optimizer as it trains the neural network in less time and more efficiently. 
#we use loss function to determine error
#categorial feature
model.fit(X_train,y_train,epochs=20,batch_size=512,verbose=1,validation_data=(X_Validation,y_Validation))
#train the model by fit method
#epochs:no of times we hv to give dataset to train the model
#batch_size:no of samples(images)per gradient update
#verbose=1 progress bar

model.evaluate(X_test, y_test)



model.save('fashion_mnist_cnn_model.h5')

#Building CNN model
cnn_model2 = keras.models.Sequential([
                         keras.layers.Conv2D(filters=32, kernel_size=3, strides=(1,1), padding='valid',activation= 'relu', input_shape=[28,28,1]),
                         keras.layers.MaxPooling2D(pool_size=(2,2)),
                         keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2,2), padding='same', activation='relu'),
                         keras.layers.MaxPooling2D(pool_size=(2,2)),
                         keras.layers.Flatten(),
                         keras.layers.Dense(units=128, activation='relu'),
                         keras.layers.Dropout(0.25),
                         keras.layers.Dense(units=256, activation='relu'),
                         keras.layers.Dropout(0.25),
                         keras.layers.Dense(units=128, activation='relu'),
                         keras.layers.Dense(units=10, activation='softmax')
                         ])
 
# complie the model
cnn_model2.compile(optimizer='adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])
 
#Train the Model
cnn_model2.fit(X_train, y_train, epochs=20, batch_size=512, verbose=1, validation_data=(X_Validation, y_Validation))
 
cnn_model2.save('fashion_mnist_cnn_model2.h5')
 
"""######## very complex model"""
 
#Building CNN model
cnn_model3 = keras.models.Sequential([
                         keras.layers.Conv2D(filters=64, kernel_size=3, strides=(1,1), padding='valid',activation= 'relu', input_shape=[28,28,1]),
                         keras.layers.MaxPooling2D(pool_size=(2,2)),
                         keras.layers.Conv2D(filters=128, kernel_size=3, strides=(2,2), padding='same', activation='relu'),
                         keras.layers.MaxPooling2D(pool_size=(2,2)),
                         keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2,2), padding='same', activation='relu'),
                         keras.layers.MaxPooling2D(pool_size=(2,2)),
                         keras.layers.Flatten(),
                         keras.layers.Dense(units=128, activation='relu'),
                         keras.layers.Dropout(0.25),
                         keras.layers.Dense(units=256, activation='relu'),
                         keras.layers.Dropout(0.5),
                         keras.layers.Dense(units=256, activation='relu'),
                         keras.layers.Dropout(0.25),                        
                         keras.layers.Dense(units=128, activation='relu'),
                         keras.layers.Dropout(0.10),                         
                         keras.layers.Dense(units=10, activation='softmax')
                         ])
 
# complie the model
cnn_model3.compile(optimizer='adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])
 
#Train the Model
cnn_model3.fit(X_train, y_train, epochs=50, batch_size=512, verbose=1, validation_data=(X_Validation, y_Validation))
 
cnn_model3.save('fashion_mnist_cnn_model3.h5')
 
cnn_model3.evaluate(X_test, y_test)